{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XOM5N5xSlGPy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HxPUCAAVlGPz"
      },
      "outputs": [],
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=3, distance_metric='minkowski', p=2):\n",
        "        self.k = k\n",
        "        self.distance_metric = distance_metric\n",
        "        self.p = p\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = np.asarray(X, dtype=float)\n",
        "        self.y_train = np.asarray(y, dtype=int)\n",
        "\n",
        "    def compute_distance(self, X_train, x):\n",
        "        X_train = np.asarray(X_train, dtype=float)\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        if self.distance_metric == 'euclidean':\n",
        "            distances = np.sqrt(np.sum((X_train - x) ** 2, axis=1))\n",
        "        elif self.distance_metric == 'manhattan':\n",
        "            distances = np.sum(np.abs(X_train - x), axis=1)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown distance metric\")\n",
        "        return distances\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = []\n",
        "        for x in X:\n",
        "            x = np.asarray(x, dtype=float)\n",
        "            distances = self.compute_distance(self.X_train, x)\n",
        "            k_indices = distances.argsort()[:self.k]\n",
        "            k_nearest_labels = self.y_train[k_indices]\n",
        "            counts = np.bincount(k_nearest_labels)\n",
        "            y_pred.append(np.argmax(counts))\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        y_proba = []\n",
        "        for x in X:\n",
        "            x = np.asarray(x, dtype=float)\n",
        "            # Compute distances to all training points\n",
        "            distances = self.compute_distance(self.X_train, x)\n",
        "\n",
        "            # Get indices of the k nearest neighbors\n",
        "            k_indices = distances.argsort()[:self.k]\n",
        "\n",
        "            # Get labels of the k nearest neighbors\n",
        "            k_nearest_labels = self.y_train[k_indices]\n",
        "\n",
        "            # Get distances of the k nearest neighbors\n",
        "            k_nearest_distances = distances[k_indices]\n",
        "\n",
        "            # Compute weights based on distances (inverse of distances)\n",
        "            weights = 1 / (k_nearest_distances + 1e-5)\n",
        "\n",
        "            # For binary classification, calculate probabilities of each class\n",
        "            class_1_weighted_sum = np.sum(weights[k_nearest_labels == 1])\n",
        "            class_0_weighted_sum = np.sum(weights[k_nearest_labels == 0])\n",
        "            total_weighted_sum = class_1_weighted_sum + class_0_weighted_sum\n",
        "\n",
        "            # Probability of class 1 (positive class)\n",
        "            proba_class_1 = class_1_weighted_sum / total_weighted_sum\n",
        "\n",
        "            # Append the probability of class 1\n",
        "            y_proba.append(proba_class_1)\n",
        "\n",
        "        return np.array(y_proba)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "M9lennCulGP0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_data(train_path, test_path):\n",
        "    # Load datasets\n",
        "    train_data = pd.read_csv(train_path)\n",
        "    test_data = pd.read_csv(test_path)\n",
        "\n",
        "    # Save test IDs for future use\n",
        "    test_ids = test_data['id'].values\n",
        "\n",
        "    # Drop unwanted columns\n",
        "    train_data.drop(['CustomerId', 'Surname'], axis=1, inplace=True)\n",
        "    test_data.drop(['CustomerId', 'Surname'], axis=1, inplace=True)\n",
        "\n",
        "    # Combine the two datasets to apply transformations consistently\n",
        "    combined_data = pd.concat([train_data, test_data], keys=['train', 'test'])\n",
        "\n",
        "    # One-hot encode categorical features and drop the first category\n",
        "    combined_data = pd.get_dummies(combined_data, columns=['Geography', 'Gender'], drop_first=True)\n",
        "\n",
        "    # Separate the data again into training and test datasets\n",
        "    train_data_processed = combined_data.xs('train', level=0)\n",
        "    test_data_processed = combined_data.xs('test', level=0)\n",
        "\n",
        "    # Identify numerical features for scaling\n",
        "    numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts',\n",
        "                          'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
        "\n",
        "    # Fill missing numerical values with the mean\n",
        "    train_data_processed[numerical_features] = train_data_processed[numerical_features].fillna(train_data_processed[numerical_features].mean())\n",
        "    test_data_processed[numerical_features] = test_data_processed[numerical_features].fillna(test_data_processed[numerical_features].mean())\n",
        "\n",
        "    # Standardize the numerical features (scaling based on training data statistics)\n",
        "    scaler = StandardScaler()\n",
        "    train_data_processed[numerical_features] = scaler.fit_transform(train_data_processed[numerical_features])\n",
        "    test_data_processed[numerical_features] = scaler.transform(test_data_processed[numerical_features])\n",
        "\n",
        "    # Separate target variable 'Exited' from training data\n",
        "    y_train = train_data_processed['Exited'].values.astype(int)\n",
        "    X_train = train_data_processed.drop(['id', 'Exited'], axis=1).values\n",
        "\n",
        "    # Prepare test data for prediction (drop 'Exited' column if it exists in the test data)\n",
        "    X_test = test_data_processed.drop(['id', 'Exited'], axis=1, errors='ignore').values\n",
        "\n",
        "    return X_train, y_train, X_test, test_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OjfVITpJlGP0"
      },
      "outputs": [],
      "source": [
        "def cross_validate(X, y, knn, n_splits=5):\n",
        "    fold_size = len(y) // n_splits\n",
        "    indices = np.arange(len(y))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    auc_scores = []\n",
        "\n",
        "    for fold in range(n_splits):\n",
        "        val_indices = indices[fold * fold_size:(fold + 1) * fold_size]\n",
        "        train_indices = np.setdiff1d(indices, val_indices)\n",
        "        X_train_fold, X_val_fold = X[train_indices], X[val_indices]\n",
        "        y_train_fold, y_val_fold = y[train_indices], y[val_indices]\n",
        "        knn.fit(X_train_fold, y_train_fold)\n",
        "        y_val_proba = knn.predict_proba(X_val_fold)\n",
        "        auc = roc_auc_manual(y_val_fold, y_val_proba)\n",
        "        auc_scores.append(auc)\n",
        "\n",
        "    return auc_scores\n",
        "\n",
        "\n",
        "def roc_auc_manual(y_true, y_pred):\n",
        "\n",
        "    sorted_indices = np.argsort(y_pred)[::-1]\n",
        "    y_true_sorted = y_true[sorted_indices]\n",
        "\n",
        "\n",
        "    pos = np.sum(y_true == 1)\n",
        "    neg = np.sum(y_true == 0)\n",
        "\n",
        "\n",
        "    tp_cumsum = np.cumsum(y_true_sorted)\n",
        "    fp_cumsum = np.cumsum(1 - y_true_sorted)\n",
        "\n",
        "    auc = np.sum(tp_cumsum[y_true_sorted == 0]) / (pos * neg)\n",
        "\n",
        "    return auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Mb42in3lGP0",
        "outputId": "03cfddc4-799e-4150-93a3-64fa60294a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-e40b4af10543>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data_processed[numerical_features] = train_data_processed[numerical_features].fillna(train_data_processed[numerical_features].mean())\n",
            "<ipython-input-18-e40b4af10543>:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data_processed[numerical_features] = test_data_processed[numerical_features].fillna(test_data_processed[numerical_features].mean())\n",
            "<ipython-input-18-e40b4af10543>:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data_processed[numerical_features] = scaler.fit_transform(train_data_processed[numerical_features])\n",
            "<ipython-input-18-e40b4af10543>:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data_processed[numerical_features] = scaler.transform(test_data_processed[numerical_features])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k=20, distance_metric=euclidean, CV Score=0.9101\n",
            "k=20, distance_metric=manhattan, CV Score=0.9089\n",
            "k=21, distance_metric=euclidean, CV Score=0.9091\n",
            "k=21, distance_metric=manhattan, CV Score=0.9087\n",
            "k=22, distance_metric=euclidean, CV Score=0.9117\n",
            "k=22, distance_metric=manhattan, CV Score=0.9094\n",
            "k=23, distance_metric=euclidean, CV Score=0.9114\n",
            "k=23, distance_metric=manhattan, CV Score=0.9106\n",
            "\n",
            "Best Parameters: {'k': 22, 'distance_metric': 'euclidean'}\n",
            "Best CV Score: 0.9117\n"
          ]
        }
      ],
      "source": [
        "X, y, X_test, test_ids = preprocess_data('train.csv', 'test.csv')\n",
        "k_values = [22]\n",
        "distance_metrics = ['euclidean', 'manhattan']\n",
        "best_score = 0\n",
        "for k in k_values:\n",
        "    for distance_metric in distance_metrics:\n",
        "            knn = KNN(k=k, distance_metric=distance_metric)\n",
        "            cv_scores = cross_validate(X, y, knn)\n",
        "            mean_score = np.mean(cv_scores)\n",
        "            print(f\"k={k}, distance_metric={distance_metric}, CV Score={mean_score:.4f}\")\n",
        "            if mean_score > best_score:\n",
        "                best_score = mean_score\n",
        "                best_params = {'k': k, 'distance_metric': distance_metric}\n",
        "\n",
        "print(\"\\nBest Parameters:\", best_params)\n",
        "print(f\"Best CV Score: {best_score:.4f}\")\n",
        "\n",
        "\n",
        "knn = KNN(k=best_params['k'], distance_metric=best_params['distance_metric'])\n",
        "\n",
        "knn.fit(X, y)\n",
        "\n",
        "test_predictions = knn.predict_proba(X_test)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'Exited': test_predictions\n",
        "})\n",
        "submission.to_csv('submissions.csv', index=False)\n",
        "submission_csv = pd.read_csv('submissions.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs506",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}